# Awesome Multimodal Dataset Distillation

   
Awesome Multimodal Dataset Distillation provides the most comprehensive and detailed information on the Multimodal Dataset Distillation field.

Multimodal dataset distillation is the task of synthesizing a small multimodal dataset such that models trained on it achieve high performance on the original large dataset. A multimodal dataset distillation algorithm takes as input a large real multimodal dataset to be distilled (training set), and outputs a small synthetic distilled dataset, which is evaluated via testing models trained on this distilled dataset on a separate real dataset (validation/test set). A good small distilled multimodal dataset is not only useful in dataset understanding, but has various applications (e.g., continual learning, privacy, neural architecture search, etc.). This task extends the concept of dataset distillation to multiple modalities, allowing for more comprehensive and efficient learning across different types of data.

In recent years, multimodal dataset distillation has gained increasing attention in the research community, across many institutes and labs. More papers are now being published each year. These wonderful researches have been constantly improving multimodal dataset distillation and exploring its various variants and applications.

This project is curated and maintained by [Jongoh Jeong](https://github.com/andyj1).

<!-- ## How to submit a pull request? -->
<!-- ğŸŒ [Project Page](#)
ğŸ“¦ [Code](#)
ğŸ“– [bibtex](#) -->

<!-- ## Latest Updates

[YYYY/MM/DD] Paper Title 1 (Author Names, Conference YYYY) [ğŸŒ](#) [ğŸ“–](#)
[YYYY/MM/DD] Paper Title 2 (Author Names, Conference YYYY) [ğŸ“–](#)
[YYYY/MM/DD] Paper Title 3 (Author Names et al., YYYY) [ğŸŒ](#) [ğŸ“–](#)
[YYYY/MM/DD] Paper Title 4 (Author Names et al., Conference YYYY) [ğŸ“–](#)
[YYYY/MM/DD] Paper Title 5 (Author Names et al., Conference YYYY) [ğŸŒ](#) [ğŸ“–](#) -->

## Contents

- [Awesome Image-only Dataset Distillation](#awesome-dataset-distillation)
- [Multimodal: Vision-Language Distillation](#vision-language-distillation)
- [Multimodal: Audio-Visual Distillation](#audio-visual-distillation)
<!-- - [Generative Multimodal Distillation](#generative-multimodal-distillation) -->

<!-- ### Applications

- [Continual Learning](#continual-learning)
- [Privacy](#privacy)
- [Medical](#medical)
- [Federated Learning](#federated-learning)
- [Robotics](#robotics)
- [Autonomous Driving](#autonomous-driving)
- [Recommendation Systems](#recommendation-systems)
- [Robustness](#robustness)
- [Fairness](#fairness) -->

## Main

### Awesome Image-only Dataset Distillation
- [GitHub](https://github.com/Guang000/Awesome-Dataset-Distillation?tab=readme-ov-file) [ğŸŒ]

### Multimodal: Vision-Language
- [Vision-Language Dataset Distillation](https://arxiv.org/abs/2308.07545)Â (Xindi Wu et al., TMLR 2024)Â [ğŸ“–] [Github](https://github.com/princetonvisualai/multimodal_dataset_distillation) [ğŸŒ]

- [Scaling up dataset distillation to imagenet1k with constant memory](https://proceedings.mlr.press/v202/cui23e/cui23e.pdf) (ICML 2023) TESLA [ğŸ“–] [Github](https://github.com/justincui03/tesla) [ğŸŒ]

- [Low-Rank Similarity Mining for Multimodal Dataset Distillation](https://arxiv.org/abs/2406.03793)Â (Yue Xu et al., ICML 2024)Â [ğŸ“–]Â [Github](https://github.com/silicx/LoRS_Distill) [ğŸŒ]

- [Beyond Modality Collapse: Representations Blending for Multimodal Dataset Distillation](https://arxiv.org/abs/2505.14705)Â (Xin Zhang et al., 2025) [ğŸ“–]

- [Multi-Modal Dataset Distillation in the Wild](https://arxiv.org/pdf/2506.01586v1) (Dang et al, 2025) [ğŸ“–] 

- [LeveragingÂ Multi-ModalÂ Information to EnhanceÂ Dataset Distillation](https://arxiv.org/abs/2505.08605) (Li et al, 2025) [ğŸ“–]

### Multimodal: Audio-Visual
- [Audio-Visual Dataset Distillation](https://openreview.net/forum?id=IJlbuSrXmk)Â (Saksham Singh Kushwaha et al., TMLR 2024)Â [ğŸ“–]Â [Github](https://github.com/sakshamsingh1/AVDD) [ğŸŒ]


<!-- ### Generative Multimodal Distillation -->
<!-- ## Applications

### Continual Learning

Paper Title 1 (Author Names et al., Conference YYYY) [ğŸ“–](#)
Paper Title 2 (Author Names et al., Conference YYYY) [ğŸŒ](#) [ğŸ“–](#)

### Privacy

Paper Title 1 (Author Names et al., Conference YYYY) [ğŸ“–](#)
Paper Title 2 (Author Names et al., Conference YYYY) [ğŸŒ](#) [ğŸ“–](#)

### Medical

Paper Title 1 (Author Names et al., Conference YYYY) [ğŸ“–](#)
Paper Title 2 (Author Names et al., Conference YYYY) [ğŸŒ](#) [ğŸ“–](#)

### Federated Learning

Paper Title 1 (Author Names et al., Conference YYYY) [ğŸ“–](#)
Paper Title 2 (Author Names et al., Conference YYYY) [ğŸŒ](#) [ğŸ“–](#)

### Robotics

Paper Title 1 (Author Names et al., Conference YYYY) [ğŸ“–](#)
Paper Title 2 (Author Names et al., Conference YYYY) [ğŸŒ](#) [ğŸ“–](#)

### Autonomous Driving

Paper Title 1 (Author Names et al., Conference YYYY) [ğŸ“–](#)
Paper Title 2 (Author Names et al., Conference YYYY) [ğŸŒ](#) [ğŸ“–](#)

### Recommendation Systems

Paper Title 1 (Author Names et al., Conference YYYY) [ğŸ“–](#)
Paper Title 2 (Author Names et al., Conference YYYY) [ğŸŒ](#) [ğŸ“–](#)

### Robustness

Paper Title 1 (Author Names et al., Conference YYYY) [ğŸ“–](#)
Paper Title 2 (Author Names et al., Conference YYYY) [ğŸŒ](#) [ğŸ“–](#)

### Fairness

Paper Title 1 (Author Names et al., Conference YYYY) [ğŸ“–](#)
Paper Title 2 (Author Names et al., Conference YYYY) [ğŸŒ](#) [ğŸ“–](#) -->



## Star History
<picture>
  <source
    media="(prefers-color-scheme: dark)"
    srcset="
      https://api.star-history.com/svg?repos=andyj1/Awesome-Multimodal-Dataset-Distillation&type=Date&theme=dark
    "
  />
  <source
    media="(prefers-color-scheme: light)"
    srcset="
      https://api.star-history.com/svg?repos=andyj1/Awesome-Multimodal-Dataset-Distillation&type=Date
    "
  />
  <img
    alt="Star History Chart"
    src="https://api.star-history.com/svg?repos=andyj1/Awesome-Multimodal-Dataset-Distillation&type=Date"
  />
</picture>


## Citing this repository
If you find this project useful for your research, please use the following BibTeX entry.
```
@misc{jeong2025awesome,
  author={Jeong, Jongoh},
  title={Awesome Multimodal Dataset Distillation},
  howpublished={\url{https://github.com/andyj1/Awesome-Multimodal-Dataset-Distillation}},
  year={2025}
}
```

## Acknowledgments
