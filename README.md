# Awesome Multimodal Dataset Distillation

   
Awesome Multimodal Dataset Distillation provides the most comprehensive and detailed information on the Multimodal Dataset Distillation field.

Multimodal dataset distillation is the task of synthesizing a small multimodal dataset such that models trained on it achieve high performance on the original large dataset. A multimodal dataset distillation algorithm takes as input a large real multimodal dataset to be distilled (training set), and outputs a small synthetic distilled dataset, which is evaluated via testing models trained on this distilled dataset on a separate real dataset (validation/test set). A good small distilled multimodal dataset is not only useful in dataset understanding, but has various applications (e.g., continual learning, privacy, neural architecture search, etc.). This task extends the concept of dataset distillation to multiple modalities, allowing for more comprehensive and efficient learning across different types of data.

In recent years, multimodal dataset distillation has gained increasing attention in the research community, across many institutes and labs. More papers are now being published each year. These wonderful researches have been constantly improving multimodal dataset distillation and exploring its various variants and applications.

This project is curated and maintained by [andyj1](https://github.com/andyj1).

<!-- ## How to submit a pull request? -->
<!-- ğŸŒ [Project Page](#)
ğŸ“¦ [Code](#)
ğŸ“– [bibtex](#) -->

<!-- ## Latest Updates

[YYYY/MM/DD] Paper Title 1 (Author Names, Conference YYYY) [ğŸŒ](#) [ğŸ“–](#)
[YYYY/MM/DD] Paper Title 2 (Author Names, Conference YYYY) [ğŸ“–](#)
[YYYY/MM/DD] Paper Title 3 (Author Names et al., YYYY) [ğŸŒ](#) [ğŸ“–](#)
[YYYY/MM/DD] Paper Title 4 (Author Names et al., Conference YYYY) [ğŸ“–](#)
[YYYY/MM/DD] Paper Title 5 (Author Names et al., Conference YYYY) [ğŸŒ](#) [ğŸ“–](#) -->

## Contents

- [Awesome Image-only Dataset Distillation](https://github.com/Guang000/Awesome-Dataset-Distillation?tab=readme-ov-file)
- [Multimodal: Vision-Language](#vision-language-distillation)
- [Multimodal: Audio-Visual](#audio-visual-distillation)
<!-- - [Generative Multimodal Distillation](#generative-multimodal-distillation) -->

<!-- ### Applications

- [Continual Learning](#continual-learning)
- [Privacy](#privacy)
- [Medical](#medical)
- [Federated Learning](#federated-learning)
- [Robotics](#robotics)
- [Autonomous Driving](#autonomous-driving)
- [Recommendation Systems](#recommendation-systems)
- [Robustness](#robustness)
- [Fairness](#fairness) -->

## Papers

### Awesome Image-only Dataset Distillation
- [GitHub](https://github.com/Guang000/Awesome-Dataset-Distillation?tab=readme-ov-file) [ğŸŒ]

### Multimodal: Vision-Language
- [Vision-Language Dataset Distillation ğŸ“–](https://arxiv.org/abs/2308.07545) (Xindi Wu et al., TMLR 2024)Â [GitHub ğŸŒ](https://github.com/princetonvisualai/multimodal_dataset_distillation)

- [Scaling up dataset distillation to imagenet1k with constant memory ğŸ“–](https://proceedings.mlr.press/v202/cui23e/cui23e.pdf) (ICML 2023) TESLA  [GitHub ğŸŒ](https://github.com/justincui03/tesla)

- [Low-Rank Similarity Mining for Multimodal Dataset Distillation ğŸ“–](https://arxiv.org/abs/2406.03793)Â (Yue Xu et al., ICML 2024)Â [GitHub ğŸŒ](https://github.com/silicx/LoRS_Distill)

- [Beyond Modality Collapse: Representations Blending for Multimodal Dataset Distillation ğŸ“–](https://arxiv.org/abs/2505.14705)Â (Xin Zhang et al., arXiv 2025)

- [Multi-Modal Dataset Distillation in the Wild ğŸ“–](https://arxiv.org/pdf/2506.01586v1) (Dang et al, arXiv 2025)

- [LeveragingÂ Multi-ModalÂ Information to EnhanceÂ Dataset Distillation ğŸ“–](https://arxiv.org/abs/2505.08605) (Li et al, arXiv 2025)

- [Dataset Distillation via Vision-Language Category Prototype ğŸ“–](https://arxiv.org/pdf/2506.23580) (Zou et al, ICCV 2025)

### Multimodal: Audio-Visual
- [Audio-Visual Dataset Distillation ğŸ“–](https://openreview.net/forum?id=IJlbuSrXmk)Â (Saksham Singh Kushwaha et al., TMLR 2024)Â [GitHub ğŸŒ](https://github.com/sakshamsingh1/AVDD)


<!-- ### Generative Multimodal Distillation -->
<!-- ## Applications

### Continual Learning

Paper Title 1 (Author Names et al., Conference YYYY) [ğŸ“–](#)
Paper Title 2 (Author Names et al., Conference YYYY) [ğŸŒ](#) [ğŸ“–](#)

### Privacy

Paper Title 1 (Author Names et al., Conference YYYY) [ğŸ“–](#)
Paper Title 2 (Author Names et al., Conference YYYY) [ğŸŒ](#) [ğŸ“–](#)

### Medical

Paper Title 1 (Author Names et al., Conference YYYY) [ğŸ“–](#)
Paper Title 2 (Author Names et al., Conference YYYY) [ğŸŒ](#) [ğŸ“–](#)

### Federated Learning

Paper Title 1 (Author Names et al., Conference YYYY) [ğŸ“–](#)
Paper Title 2 (Author Names et al., Conference YYYY) [ğŸŒ](#) [ğŸ“–](#)

### Robotics

Paper Title 1 (Author Names et al., Conference YYYY) [ğŸ“–](#)
Paper Title 2 (Author Names et al., Conference YYYY) [ğŸŒ](#) [ğŸ“–](#)

### Autonomous Driving

Paper Title 1 (Author Names et al., Conference YYYY) [ğŸ“–](#)
Paper Title 2 (Author Names et al., Conference YYYY) [ğŸŒ](#) [ğŸ“–](#)

### Recommendation Systems

Paper Title 1 (Author Names et al., Conference YYYY) [ğŸ“–](#)
Paper Title 2 (Author Names et al., Conference YYYY) [ğŸŒ](#) [ğŸ“–](#)

### Robustness

Paper Title 1 (Author Names et al., Conference YYYY) [ğŸ“–](#)
Paper Title 2 (Author Names et al., Conference YYYY) [ğŸŒ](#) [ğŸ“–](#)

### Fairness

Paper Title 1 (Author Names et al., Conference YYYY) [ğŸ“–](#)
Paper Title 2 (Author Names et al., Conference YYYY) [ğŸŒ](#) [ğŸ“–](#) -->



## Star History
<picture>
  <source
    media="(prefers-color-scheme: dark)"
    srcset="
      https://api.star-history.com/svg?repos=andyj1/Awesome-Multimodal-Dataset-Distillation&type=Date&theme=dark
    "
  />
  <source
    media="(prefers-color-scheme: light)"
    srcset="
      https://api.star-history.com/svg?repos=andyj1/Awesome-Multimodal-Dataset-Distillation&type=Date
    "
  />
  <img
    alt="Star History Chart"
    src="https://api.star-history.com/svg?repos=andyj1/Awesome-Multimodal-Dataset-Distillation&type=Date"
  />
</picture>


## Citing this repository
If you find this project useful for your research, please use the following BibTeX entry.
```
@misc{jeong2025awesome,
  author={Jeong, Jongoh},
  title={Awesome Multimodal Dataset Distillation},
  howpublished={\url{https://github.com/andyj1/Awesome-Multimodal-Dataset-Distillation}},
  year={2025}
}
```

## Acknowledgments
