# Awesome Multimodal Dataset Distillation

<div align="center">
  <img src="./assets/MDD_logo.png" width="200"/>
  <div>&nbsp;</div>
  <div align="center">
    <b><font size="3">Vision-Language</font></b>
    &nbsp;&nbsp;
    Â·
    &nbsp;&nbsp;&nbsp;
    <b><font size="3">Audio-Visual</font></b>
  </div>
  <div>&nbsp;</div>
</div>

Awesome Multimodal Dataset Distillation provides the most comprehensive and detailed information on the Multimodal Dataset Distillation field.

Multimodal dataset distillation is the task of synthesizing a small multimodal dataset such that models trained on it achieve high performance on the original large dataset. A multimodal dataset distillation algorithm takes as input a large real multimodal dataset to be distilled (training set), and outputs a small synthetic distilled dataset, which is evaluated via testing models trained on this distilled dataset on a separate real dataset (validation/test set). A good small distilled multimodal dataset is not only useful in dataset understanding, but has various applications (e.g., continual learning, privacy, neural architecture search, etc.). This task extends the concept of dataset distillation to multiple modalities, allowing for more comprehensive and efficient learning across different types of data.

In recent years, multimodal dataset distillation has gained increasing attention in the research community, across many institutes and labs. More papers are now being published each year. These wonderful researches have been constantly improving multimodal dataset distillation and exploring its various variants and applications.

This project is curated and maintained by [andyj1](https://github.com/andyj1).

<!-- ## How to submit a pull request? -->
<!-- ğŸŒ [Project Page](#)
ğŸ“¦ [Code](#)
ğŸ“– [bibtex](#) -->

<!-- ## Latest Updates

[YYYY/MM/DD] Paper Title 1 (Author Names, Conference YYYY) [ğŸŒ](#) [ğŸ“–](#)
[YYYY/MM/DD] Paper Title 2 (Author Names, Conference YYYY) [ğŸ“–](#)
[YYYY/MM/DD] Paper Title 3 (Author Names et al., YYYY) [ğŸŒ](#) [ğŸ“–](#)
[YYYY/MM/DD] Paper Title 4 (Author Names et al., Conference YYYY) [ğŸ“–](#)
[YYYY/MM/DD] Paper Title 5 (Author Names et al., Conference YYYY) [ğŸŒ](#) [ğŸ“–](#) -->

## Contents

- [Awesome Image-only Dataset Distillation](https://github.com/Guang000/Awesome-Dataset-Distillation?tab=readme-ov-file)
- [Multimodal: Vision-Language](#vision-language-distillation)
- [Multimodal: Audio-Visual](#audio-visual-distillation)
<!-- - [Generative Multimodal Distillation](#generative-multimodal-distillation) -->

<!-- ### Applications

- [Continual Learning](#continual-learning)
- [Privacy](#privacy)
- [Medical](#medical)
- [Federated Learning](#federated-learning)
- [Robotics](#robotics)
- [Autonomous Driving](#autonomous-driving)
- [Recommendation Systems](#recommendation-systems)
- [Robustness](#robustness)
- [Fairness](#fairness) -->

## Papers

### Awesome Image-only Dataset Distillation
- [GitHub](https://github.com/Guang000/Awesome-Dataset-Distillation?tab=readme-ov-file) [ğŸŒ]

- [LeveragingÂ Multi-ModalÂ Information to EnhanceÂ Dataset Distillation ğŸ“–](https://arxiv.org/abs/2505.08605) Caption Combination (Li et al, arXiv 2025)
  
- [Dataset Distillation via Vision-Language Category Prototype ğŸ“–](https://arxiv.org/pdf/2506.23580) (Zou et al, ICCV 2025)

- [Hyperbolic Dataset Distillation ğŸ“–](https://github.com/Guang000/Awesome-Dataset-Distillation/blob/main/citations/li2025hdd.txt) (Wenyuan Li & Guang Li et al., NeurIPS 2025)
    
### Multimodal: Vision-Language
- [Vision-Language Dataset Distillation ğŸ“–](https://arxiv.org/abs/2308.07545) VL-Distill (Xindi Wu et al., TMLR 2024)Â [GitHub ğŸŒ](https://github.com/princetonvisualai/multimodal_dataset_distillation)

- [Scaling up dataset distillation to imagenet1k with constant memory ğŸ“–](https://proceedings.mlr.press/v202/cui23e/cui23e.pdf) TESLA (Cui et al., ICML 2023)  [GitHub ğŸŒ](https://github.com/justincui03/tesla)

- [Low-Rank Similarity Mining for Multimodal Dataset Distillation ğŸ“–](https://arxiv.org/abs/2406.03793)Â LoRS (Yue Xu et al., ICML 2024)Â [GitHub ğŸŒ](https://github.com/silicx/LoRS_Distill)

- [Beyond Modality Collapse: Representations Blending for Multimodal Dataset Distillation ğŸ“–](https://arxiv.org/abs/2505.14705)Â RepBlend (Xin Zhang et al., NeurIPS 2025)

- [Efficient Multimodal Dataset Distillation via Generative Models ğŸ“–](https://arxiv.org/pdf/2509.15472) EDGE (Zhenghao Zhao et al., NeurIPS 2025) [Github ğŸŒ](https://github.com/ichbill/EDGE)

- [Multi-Modal Dataset Distillation in the Wild ğŸ“–](https://arxiv.org/pdf/2506.01586v1) MDW (Dang et al, arXiv 2025)

- [Multimodal dataset distillation made simple by prototype-guided data synthesis ğŸ“–](https://openreview.net/pdf/2d64e6404555760e7b759d1c5038892464071db8.pdf) PDS (Choi et al. ICLR 2026)

- [Multimodal Dataset Distillation via Phased Teacher Models ğŸ“–](https://openreview.net/pdf?id=Me4AON8160) PTM-ST (ICLR 2026)

- [Understanding Dataset Distillation via Spectral Filtering](https://openreview.net/forum?id=0h5ohpUGY4) UniDD (ICLR 2026)
  
- [Efficient Multi-modal Dataset Distillation via Analytic Parameter Matching](https://openreview.net/forum?id=Fxz0aaGSNY)) APM (ICLR 2026 reject)

  

### Multimodal: Audio-Visual
- [Audio-Visual Dataset Distillation ğŸ“–](https://openreview.net/forum?id=IJlbuSrXmk)Â (Saksham Singh Kushwaha et al., TMLR 2024)Â [GitHub ğŸŒ](https://github.com/sakshamsingh1/AVDD)

- [Decoupled Audio-Visual Dataset Distillation](https://arxiv.org/pdf/2511.17890) (Li et al., arXiv 2025) 


<!-- ### Generative Multimodal Distillation -->
<!-- ## Applications

### Continual Learning

Paper Title 1 (Author Names et al., Conference YYYY) [ğŸ“–](#)
Paper Title 2 (Author Names et al., Conference YYYY) [ğŸŒ](#) [ğŸ“–](#)

### Privacy

Paper Title 1 (Author Names et al., Conference YYYY) [ğŸ“–](#)
Paper Title 2 (Author Names et al., Conference YYYY) [ğŸŒ](#) [ğŸ“–](#)

### Medical

Paper Title 1 (Author Names et al., Conference YYYY) [ğŸ“–](#)
Paper Title 2 (Author Names et al., Conference YYYY) [ğŸŒ](#) [ğŸ“–](#)

### Federated Learning

Paper Title 1 (Author Names et al., Conference YYYY) [ğŸ“–](#)
Paper Title 2 (Author Names et al., Conference YYYY) [ğŸŒ](#) [ğŸ“–](#)

### Robotics

Paper Title 1 (Author Names et al., Conference YYYY) [ğŸ“–](#)
Paper Title 2 (Author Names et al., Conference YYYY) [ğŸŒ](#) [ğŸ“–](#)

### Autonomous Driving

Paper Title 1 (Author Names et al., Conference YYYY) [ğŸ“–](#)
Paper Title 2 (Author Names et al., Conference YYYY) [ğŸŒ](#) [ğŸ“–](#)

### Recommendation Systems

Paper Title 1 (Author Names et al., Conference YYYY) [ğŸ“–](#)
Paper Title 2 (Author Names et al., Conference YYYY) [ğŸŒ](#) [ğŸ“–](#)

### Robustness

Paper Title 1 (Author Names et al., Conference YYYY) [ğŸ“–](#)
Paper Title 2 (Author Names et al., Conference YYYY) [ğŸŒ](#) [ğŸ“–](#)

### Fairness

Paper Title 1 (Author Names et al., Conference YYYY) [ğŸ“–](#)
Paper Title 2 (Author Names et al., Conference YYYY) [ğŸŒ](#) [ğŸ“–](#) -->



## Star History
<picture>
  <source
    media="(prefers-color-scheme: dark)"
    srcset="
      https://api.star-history.com/svg?repos=andyj1/Awesome-Multimodal-Dataset-Distillation&type=Date&theme=dark
    "
  />
  <source
    media="(prefers-color-scheme: light)"
    srcset="
      https://api.star-history.com/svg?repos=andyj1/Awesome-Multimodal-Dataset-Distillation&type=Date
    "
  />
  <img
    alt="Star History Chart"
    src="https://api.star-history.com/svg?repos=andyj1/Awesome-Multimodal-Dataset-Distillation&type=Date"
  />
</picture>


## Citing this repository
If you find this project useful for your research, please use the following BibTeX entry.
```
@misc{jeong2025awesome,
  author={Jeong, Jongoh},
  title={Awesome Multimodal Dataset Distillation},
  howpublished={\url{https://github.com/andyj1/Awesome-Multimodal-Dataset-Distillation}},
  year={2025}
}
```

## Acknowledgments
