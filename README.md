# Awesome Multimodal Dataset Distillation

   
Awesome Multimodal Dataset Distillation provides the most comprehensive and detailed information on the Multimodal Dataset Distillation field.

Multimodal dataset distillation is the task of synthesizing a small multimodal dataset such that models trained on it achieve high performance on the original large dataset. A multimodal dataset distillation algorithm takes as input a large real multimodal dataset to be distilled (training set), and outputs a small synthetic distilled dataset, which is evaluated via testing models trained on this distilled dataset on a separate real dataset (validation/test set). A good small distilled multimodal dataset is not only useful in dataset understanding, but has various applications (e.g., continual learning, privacy, neural architecture search, etc.). This task extends the concept of dataset distillation to multiple modalities, allowing for more comprehensive and efficient learning across different types of data.

In recent years, multimodal dataset distillation has gained increasing attention in the research community, across many institutes and labs. More papers are now being published each year. These wonderful researches have been constantly improving multimodal dataset distillation and exploring its various variants and applications.

This project is curated and maintained by [andyj1](https://github.com/andyj1).

<!-- ## How to submit a pull request? -->
<!-- 🌐 [Project Page](#)
📦 [Code](#)
📖 [bibtex](#) -->

<!-- ## Latest Updates

[YYYY/MM/DD] Paper Title 1 (Author Names, Conference YYYY) [🌐](#) [📖](#)
[YYYY/MM/DD] Paper Title 2 (Author Names, Conference YYYY) [📖](#)
[YYYY/MM/DD] Paper Title 3 (Author Names et al., YYYY) [🌐](#) [📖](#)
[YYYY/MM/DD] Paper Title 4 (Author Names et al., Conference YYYY) [📖](#)
[YYYY/MM/DD] Paper Title 5 (Author Names et al., Conference YYYY) [🌐](#) [📖](#) -->

## Contents

- [Awesome Image-only Dataset Distillation](https://github.com/Guang000/Awesome-Dataset-Distillation?tab=readme-ov-file)
- [Multimodal: Vision-Language](#vision-language-distillation)
- [Multimodal: Audio-Visual](#audio-visual-distillation)
<!-- - [Generative Multimodal Distillation](#generative-multimodal-distillation) -->

<!-- ### Applications

- [Continual Learning](#continual-learning)
- [Privacy](#privacy)
- [Medical](#medical)
- [Federated Learning](#federated-learning)
- [Robotics](#robotics)
- [Autonomous Driving](#autonomous-driving)
- [Recommendation Systems](#recommendation-systems)
- [Robustness](#robustness)
- [Fairness](#fairness) -->

## Papers

### Awesome Image-only Dataset Distillation
- [GitHub](https://github.com/Guang000/Awesome-Dataset-Distillation?tab=readme-ov-file) [🌐]

### Multimodal: Vision-Language
- [Vision-Language Dataset Distillation 📖](https://arxiv.org/abs/2308.07545) (Xindi Wu et al., TMLR 2024) [GitHub 🌐](https://github.com/princetonvisualai/multimodal_dataset_distillation)

- [Scaling up dataset distillation to imagenet1k with constant memory 📖](https://proceedings.mlr.press/v202/cui23e/cui23e.pdf) (ICML 2023) TESLA  [GitHub 🌐](https://github.com/justincui03/tesla)

- [Low-Rank Similarity Mining for Multimodal Dataset Distillation 📖](https://arxiv.org/abs/2406.03793) (Yue Xu et al., ICML 2024) [GitHub 🌐](https://github.com/silicx/LoRS_Distill)

- [Beyond Modality Collapse: Representations Blending for Multimodal Dataset Distillation 📖](https://arxiv.org/abs/2505.14705) (Xin Zhang et al., arXiv 2025)

- [Multi-Modal Dataset Distillation in the Wild 📖](https://arxiv.org/pdf/2506.01586v1) (Dang et al, arXiv 2025)

- [Leveraging Multi-Modal Information to Enhance Dataset Distillation 📖](https://arxiv.org/abs/2505.08605) (Li et al, arXiv 2025)

- [Dataset Distillation via Vision-Language Category Prototype 📖](https://arxiv.org/pdf/2506.23580) (Zou et al, ICCV 2025)

### Multimodal: Audio-Visual
- [Audio-Visual Dataset Distillation 📖](https://openreview.net/forum?id=IJlbuSrXmk) (Saksham Singh Kushwaha et al., TMLR 2024) [GitHub 🌐](https://github.com/sakshamsingh1/AVDD)


<!-- ### Generative Multimodal Distillation -->
<!-- ## Applications

### Continual Learning

Paper Title 1 (Author Names et al., Conference YYYY) [📖](#)
Paper Title 2 (Author Names et al., Conference YYYY) [🌐](#) [📖](#)

### Privacy

Paper Title 1 (Author Names et al., Conference YYYY) [📖](#)
Paper Title 2 (Author Names et al., Conference YYYY) [🌐](#) [📖](#)

### Medical

Paper Title 1 (Author Names et al., Conference YYYY) [📖](#)
Paper Title 2 (Author Names et al., Conference YYYY) [🌐](#) [📖](#)

### Federated Learning

Paper Title 1 (Author Names et al., Conference YYYY) [📖](#)
Paper Title 2 (Author Names et al., Conference YYYY) [🌐](#) [📖](#)

### Robotics

Paper Title 1 (Author Names et al., Conference YYYY) [📖](#)
Paper Title 2 (Author Names et al., Conference YYYY) [🌐](#) [📖](#)

### Autonomous Driving

Paper Title 1 (Author Names et al., Conference YYYY) [📖](#)
Paper Title 2 (Author Names et al., Conference YYYY) [🌐](#) [📖](#)

### Recommendation Systems

Paper Title 1 (Author Names et al., Conference YYYY) [📖](#)
Paper Title 2 (Author Names et al., Conference YYYY) [🌐](#) [📖](#)

### Robustness

Paper Title 1 (Author Names et al., Conference YYYY) [📖](#)
Paper Title 2 (Author Names et al., Conference YYYY) [🌐](#) [📖](#)

### Fairness

Paper Title 1 (Author Names et al., Conference YYYY) [📖](#)
Paper Title 2 (Author Names et al., Conference YYYY) [🌐](#) [📖](#) -->



## Star History
<picture>
  <source
    media="(prefers-color-scheme: dark)"
    srcset="
      https://api.star-history.com/svg?repos=andyj1/Awesome-Multimodal-Dataset-Distillation&type=Date&theme=dark
    "
  />
  <source
    media="(prefers-color-scheme: light)"
    srcset="
      https://api.star-history.com/svg?repos=andyj1/Awesome-Multimodal-Dataset-Distillation&type=Date
    "
  />
  <img
    alt="Star History Chart"
    src="https://api.star-history.com/svg?repos=andyj1/Awesome-Multimodal-Dataset-Distillation&type=Date"
  />
</picture>


## Citing this repository
If you find this project useful for your research, please use the following BibTeX entry.
```
@misc{jeong2025awesome,
  author={Jeong, Jongoh},
  title={Awesome Multimodal Dataset Distillation},
  howpublished={\url{https://github.com/andyj1/Awesome-Multimodal-Dataset-Distillation}},
  year={2025}
}
```

## Acknowledgments
